{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_columns', None)\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler, PowerTransformer, QuantileTransformer, FunctionTransformer, RobustScaler, StandardScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer, KNNImputer\n",
    "\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.ensemble import EasyEnsembleClassifier, RUSBoostClassifier, BalancedBaggingClassifier, BalancedRandomForestClassifier\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, RandomForestClassifier,HistGradientBoostingClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, RidgeClassifier, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_values = ['nan', 'na','#VALUE!','missing']\n",
    "train = pd.read_csv('data/TrainingData.csv', na_values=na_values)\n",
    "test = pd.read_csv('data/testX.csv', na_values=na_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "train['mvar47'] = le.fit_transform(train['mvar47'])\n",
    "test['mvar47'] = le.transform(test['mvar47'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(['application_key', 'default_ind'], axis=1)\n",
    "y = train[['default_ind']]\n",
    "X.dropna(thresh=len(X)*0.20, axis=1, inplace=True) # 0.20 for no dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [col for col in X.columns if X[col].nunique() < 30]\n",
    "numerical_cols = [col for col in X.columns if col not in categorical_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.06%\n",
      "F1 Score: 58.03%\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0, shuffle=True, stratify=y)\n",
    "\n",
    "classifier = LogisticRegression(\n",
    "    penalty='l2',\n",
    "    C=1.0,\n",
    "    class_weight='balanced',\n",
    "    random_state=0,\n",
    "    solver='lbfgs',\n",
    "    max_iter=100,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "transformer = FunctionTransformer(func=np.log1p)\n",
    "# transformer = PowerTransformer()\n",
    "# transformer = QuantileTransformer(output_distribution='normal', random_state=0)\n",
    "X_train[numerical_cols] = transformer.fit_transform(X_train[numerical_cols])\n",
    "X_test[numerical_cols] = transformer.transform(X_test[numerical_cols])\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train[:] = scaler.fit_transform(X_train[:])\n",
    "# X_test[:] = scaler.transform(X_test[:])\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train[numerical_cols] = imputer.fit_transform(X_train[numerical_cols])\n",
    "X_test[numerical_cols] = imputer.transform(X_test[numerical_cols])\n",
    "\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "X_train[categorical_cols] = imputer.fit_transform(X_train[categorical_cols])\n",
    "X_test[categorical_cols] = imputer.transform(X_test[categorical_cols])\n",
    "\n",
    "# over_under_sampler = SMOTEENN(random_state=0, n_jobs=-1)\n",
    "# over_under_sampler = SMOTETomek(random_state=0, n_jobs=-1)\n",
    "# X_train, y_train = over_under_sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "classifier.fit(X_train, y_train.values.ravel())\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_score(y_test.values.ravel(), y_pred)*100))\n",
    "print(\"F1 Score: %.2f%%\" % (f1_score(y_test.values.ravel(), y_pred)*100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression (Unbalanced):\n",
    "\n",
    "16.97% (No transformation, no scaling, no oversampling)\n",
    "\n",
    "47.84% (No transformation, standard scaling, no oversampling)\n",
    "\n",
    "48.60% (Log transformation, no scaling, no oversampling)\n",
    "\n",
    "48.88% (Log transformation, standard scaling, no oversampling)\n",
    "\n",
    "52.41% / 53.62% (No transformation, no scaling, with SMOTEENN/SMOTETomek)\n",
    "\n",
    "55.20% / 57.62% (No transformation, standard scaling, with SMOTEENN/SMOTETomek)\n",
    "\n",
    "54.78% / 57.75% (Log transformation, no scaling, with SMOTEENN/SMOTETomek)\n",
    "\n",
    "54.84% / 57.75% (Log transformation, standard scaling, with SMOTEENN/SMOTETomek)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression (Balanced):\n",
    "\n",
    "53.83% (With balance, no transformation, no scaling, no oversampling)\n",
    "\n",
    "57.88% (With balance, no transformation, standard scaling, no oversampling)\n",
    "\n",
    "58.05% (With balance, log transformation, no scaling, no oversampling)\n",
    "\n",
    "57.97% (With balance, log transformation, standard scaling, no oversampling)\n",
    "\n",
    "53.61% / 53.62% (With balance, no transformation, no scaling, with SMOTEENN/SMOTETomek)\n",
    "\n",
    "56.89% / 57.62% (With balance, no transformation, standard scaling, with SMOTEENN/SMOTETomek)\n",
    "\n",
    "57.30% / 57.75% (With balance, log transformation, no scaling, with SMOTEENN/SMOTETomek)\n",
    "\n",
    "57.25% / 57.75% (With balance, log transformation, standard scaling, with SMOTEENN/SMOTETomek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhip\\Documents\\projects\\ida_project\\env\\lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py:336: FutureWarning: The parameter `n_jobs` has been deprecated in 0.10 and will be removed in 0.12. You can pass an nearest neighbors estimator where `n_jobs` is already set instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.05%\n",
      "F1 Score: 56.93%\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0, shuffle=True, stratify=y)\n",
    "\n",
    "classifier = GaussianNB()\n",
    "\n",
    "transformer = FunctionTransformer(func=np.log1p)\n",
    "# transformer = PowerTransformer()\n",
    "# transformer = QuantileTransformer(output_distribution='normal', random_state=0)\n",
    "X_train[numerical_cols] = transformer.fit_transform(X_train[numerical_cols])\n",
    "X_test[numerical_cols] = transformer.transform(X_test[numerical_cols])\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train[:] = scaler.fit_transform(X_train[:])\n",
    "# X_test[:] = scaler.transform(X_test[:])\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train[numerical_cols] = imputer.fit_transform(X_train[numerical_cols])\n",
    "X_test[numerical_cols] = imputer.transform(X_test[numerical_cols])\n",
    "\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "X_train[categorical_cols] = imputer.fit_transform(X_train[categorical_cols])\n",
    "X_test[categorical_cols] = imputer.transform(X_test[categorical_cols])\n",
    "\n",
    "over_under_sampler = SMOTEENN(random_state=0, n_jobs=-1)\n",
    "over_under_sampler = SMOTETomek(random_state=0, n_jobs=-1)\n",
    "X_train, y_train = over_under_sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "classifier.fit(X_train, y_train.values.ravel())\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_score(y_test.values.ravel(), y_pred)*100))\n",
    "print(\"F1 Score: %.2f%%\" % (f1_score(y_test.values.ravel(), y_pred)*100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Naive Bayes:\n",
    "\n",
    "52.31% (No transformation, no scaling, no oversampling)\n",
    "\n",
    "55.78% (No transformation, standard scaling, no oversampling)\n",
    "\n",
    "55.63% (Log transformation, no scaling, no oversampling)\n",
    "\n",
    "55.63% (Log transformation, standard scaling, no oversampling)\n",
    "\n",
    "55.63% / 51.45% (No transformation, no scaling, with SMOTEENN/SMOTETomek)\n",
    "\n",
    "56.19% / 56.52% (No transformation, standard scaling, with SMOTEENN/SMOTETomek)\n",
    "\n",
    "56.65% / 56.93% (Log transformation, no scaling, with SMOTEENN/SMOTETomek)\n",
    "\n",
    "56.60% / 56.78% (Log transformation, standard scaling, with SMOTEENN/SMOTETomek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhip\\Documents\\projects\\ida_project\\env\\lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py:336: FutureWarning: The parameter `n_jobs` has been deprecated in 0.10 and will be removed in 0.12. You can pass an nearest neighbors estimator where `n_jobs` is already set instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 69.45%\n",
      "F1 Score: 57.70%\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0, shuffle=True, stratify=y)\n",
    "\n",
    "classifier = LinearDiscriminantAnalysis()\n",
    "\n",
    "transformer = FunctionTransformer(func=np.log1p)\n",
    "# transformer = PowerTransformer()\n",
    "# transformer = QuantileTransformer(output_distribution='normal', random_state=0)\n",
    "X_train[numerical_cols] = transformer.fit_transform(X_train[numerical_cols])\n",
    "X_test[numerical_cols] = transformer.transform(X_test[numerical_cols])\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train[:] = scaler.fit_transform(X_train[:])\n",
    "# X_test[:] = scaler.transform(X_test[:])\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train[numerical_cols] = imputer.fit_transform(X_train[numerical_cols])\n",
    "X_test[numerical_cols] = imputer.transform(X_test[numerical_cols])\n",
    "\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "X_train[categorical_cols] = imputer.fit_transform(X_train[categorical_cols])\n",
    "X_test[categorical_cols] = imputer.transform(X_test[categorical_cols])\n",
    "\n",
    "# over_under_sampler = SMOTEENN(random_state=0, n_jobs=-1)\n",
    "over_under_sampler = SMOTETomek(random_state=0, n_jobs=-1)\n",
    "X_train, y_train = over_under_sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "classifier.fit(X_train, y_train.values.ravel())\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_score(y_test.values.ravel(), y_pred)*100))\n",
    "print(\"F1 Score: %.2f%%\" % (f1_score(y_test.values.ravel(), y_pred)*100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Discriminant Analysis:\n",
    "\n",
    "48.25% (No transformation, no oversampling)\n",
    "\n",
    "48.90% (Log transformation, no oversampling)\n",
    "\n",
    "54.51% / 57.72% (No transformation, with SMOTEENN/SMOTETomek)\n",
    "\n",
    "54.10% / 57.70% (Log transformation, with SMOTEENN/SMOTETomek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhip\\Documents\\projects\\ida_project\\env\\lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py:336: FutureWarning: The parameter `n_jobs` has been deprecated in 0.10 and will be removed in 0.12. You can pass an nearest neighbors estimator where `n_jobs` is already set instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 67.60%\n",
      "F1 Score: 55.74%\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0, shuffle=True, stratify=y)\n",
    "\n",
    "classifier = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "transformer = FunctionTransformer(func=np.log1p)\n",
    "# transformer = PowerTransformer()\n",
    "# transformer = QuantileTransformer(output_distribution='normal', random_state=0)\n",
    "X_train[numerical_cols] = transformer.fit_transform(X_train[numerical_cols])\n",
    "X_test[numerical_cols] = transformer.transform(X_test[numerical_cols])\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train[:] = scaler.fit_transform(X_train[:])\n",
    "# X_test[:] = scaler.transform(X_test[:])\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train[numerical_cols] = imputer.fit_transform(X_train[numerical_cols])\n",
    "X_test[numerical_cols] = imputer.transform(X_test[numerical_cols])\n",
    "\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "X_train[categorical_cols] = imputer.fit_transform(X_train[categorical_cols])\n",
    "X_test[categorical_cols] = imputer.transform(X_test[categorical_cols])\n",
    "\n",
    "over_under_sampler = SMOTEENN(random_state=0, n_jobs=-1)\n",
    "# over_under_sampler = SMOTETomek(random_state=0, n_jobs=-1)\n",
    "X_train, y_train = over_under_sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "classifier.fit(X_train, y_train.values.ravel())\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_score(y_test.values.ravel(), y_pred)*100))\n",
    "print(\"F1 Score: %.2f%%\" % (f1_score(y_test.values.ravel(), y_pred)*100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quadratic Discriminant Analysis:\n",
    "\n",
    "54.71% (No transformation, no oversampling)\n",
    "\n",
    "54.30% (Log transformation, no oversampling)\n",
    "\n",
    "54.83% / 55.11% (No transformation, with SMOTEENN/SMOTETomek)\n",
    "\n",
    "55.74% / 55.66% (Log transformation, with SMOTEENN/SMOTETomek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 69.80%\n",
      "F1 Score: 57.94%\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0, shuffle=True, stratify=y)\n",
    "\n",
    "classifier = LinearSVC(\n",
    "    dual=False,\n",
    "    C=1.0,\n",
    "    class_weight='balanced',\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "transformer = FunctionTransformer(func=np.log1p)\n",
    "# transformer = PowerTransformer()\n",
    "# transformer = QuantileTransformer(output_distribution='normal', random_state=0)\n",
    "X_train[numerical_cols] = transformer.fit_transform(X_train[numerical_cols])\n",
    "X_test[numerical_cols] = transformer.transform(X_test[numerical_cols])\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train[:] = scaler.fit_transform(X_train[:])\n",
    "# X_test[:] = scaler.transform(X_test[:])\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train[numerical_cols] = imputer.fit_transform(X_train[numerical_cols])\n",
    "X_test[numerical_cols] = imputer.transform(X_test[numerical_cols])\n",
    "\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "X_train[categorical_cols] = imputer.fit_transform(X_train[categorical_cols])\n",
    "X_test[categorical_cols] = imputer.transform(X_test[categorical_cols])\n",
    "\n",
    "# over_under_sampler = SMOTEENN(random_state=0, n_jobs=-1)\n",
    "# over_under_sampler = SMOTETomek(random_state=0, n_jobs=-1)\n",
    "# X_train, y_train = over_under_sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "classifier.fit(X_train, y_train.values.ravel())\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_score(y_test.values.ravel(), y_pred)*100))\n",
    "print(\"F1 Score: %.2f%%\" % (f1_score(y_test.values.ravel(), y_pred)*100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine (Unbalanced):\n",
    "\n",
    "19.18% (No transformation, no scaling, no oversampling)\n",
    "\n",
    "45.72% (No transformation, standard scaling, no oversampling)\n",
    "\n",
    "46.54% (Log transformation, no scaling, no oversampling)\n",
    "\n",
    "46.36% (Log transformation, standard scaling, no oversampling)\n",
    "\n",
    "52.57% / 54.02% (No transformation, no scaling, with SMOTEENN/SMOTETomek)\n",
    "\n",
    "54.91% / 57.54% (No transformation, standard scaling, with SMOTEENN/SMOTETomek)\n",
    "\n",
    "54.53% / 57.73% (Log transformation, no scaling, with SMOTEENN/SMOTETomek)\n",
    "\n",
    "54.60% / 57.63% (Log transformation, standard scaling, with SMOTEENN/SMOTETomek)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machine (Balanced):\n",
    "\n",
    "54.66% (No transformation, no scaling, no oversampling)\n",
    "\n",
    "57.73% (No transformation, standard scaling, no oversampling)\n",
    "\n",
    "57.97% (Log transformation, no scaling, no oversampling)\n",
    "\n",
    "58.01% (Log transformation, standard scaling, no oversampling)\n",
    "\n",
    "53.63% / 54.53% (No transformation, no scaling, with SMOTEENN/SMOTETomek)\n",
    "\n",
    "56.87% / 57.54% (No transformation, standard scaling, with SMOTEENN/SMOTETomek)\n",
    "\n",
    "57.04% / 57.67% (Log transformation, no scaling, with SMOTEENN/SMOTETomek)\n",
    "\n",
    "57.16% / 57.63% (Log transformation, standard scaling, with SMOTEENN/SMOTETomek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 68.11%\n",
      "F1 Score: 57.50%\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0, shuffle=True, stratify=y)\n",
    "\n",
    "classifier = RidgeClassifier(\n",
    "    alpha=1.0,\n",
    "    fit_intercept=False,\n",
    "    class_weight='balanced',\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "transformer = FunctionTransformer(func=np.log1p)\n",
    "# transformer = PowerTransformer()\n",
    "# transformer = QuantileTransformer(output_distribution='normal', random_state=0)\n",
    "X_train[numerical_cols] = transformer.fit_transform(X_train[numerical_cols])\n",
    "X_test[numerical_cols] = transformer.transform(X_test[numerical_cols])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train[:] = scaler.fit_transform(X_train[:])\n",
    "X_test[:] = scaler.transform(X_test[:])\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train[numerical_cols] = imputer.fit_transform(X_train[numerical_cols])\n",
    "X_test[numerical_cols] = imputer.transform(X_test[numerical_cols])\n",
    "\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "X_train[categorical_cols] = imputer.fit_transform(X_train[categorical_cols])\n",
    "X_test[categorical_cols] = imputer.transform(X_test[categorical_cols])\n",
    "\n",
    "# over_under_sampler = SMOTEENN(random_state=0, n_jobs=-1)\n",
    "# over_under_sampler = SMOTETomek(random_state=0, n_jobs=-1)\n",
    "# X_train, y_train = over_under_sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "classifier.fit(X_train, y_train.values.ravel())\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_score(y_test.values.ravel(), y_pred)*100))\n",
    "print(\"F1 Score: %.2f%%\" % (f1_score(y_test.values.ravel(), y_pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 68.73%\n",
      "F1 Score: 57.33%\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0, shuffle=True, stratify=y)\n",
    "\n",
    "classifier = SGDClassifier(\n",
    "    loss='hinge',\n",
    "    penalty='l1',\n",
    "    alpha=0.0001,\n",
    "    fit_intercept=False,\n",
    "    n_jobs=-1,\n",
    "    random_state=0,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "\n",
    "# transformer = FunctionTransformer(func=np.log1p)\n",
    "# transformer = PowerTransformer()\n",
    "# transformer = QuantileTransformer(output_distribution='normal', random_state=0)\n",
    "# X_train[numerical_cols] = transformer.fit_transform(X_train[numerical_cols])\n",
    "# X_test[numerical_cols] = transformer.transform(X_test[numerical_cols])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train[:] = scaler.fit_transform(X_train[:])\n",
    "X_test[:] = scaler.transform(X_test[:])\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train[numerical_cols] = imputer.fit_transform(X_train[numerical_cols])\n",
    "X_test[numerical_cols] = imputer.transform(X_test[numerical_cols])\n",
    "\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "X_train[categorical_cols] = imputer.fit_transform(X_train[categorical_cols])\n",
    "X_test[categorical_cols] = imputer.transform(X_test[categorical_cols])\n",
    "\n",
    "# over_under_sampler = SMOTEENN(random_state=0, n_jobs=-1)\n",
    "# over_under_sampler = SMOTETomek(random_state=0, n_jobs=-1)\n",
    "# X_train, y_train = over_under_sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "classifier.fit(X_train, y_train.values.ravel())\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_score(y_test.values.ravel(), y_pred)*100))\n",
    "print(\"F1 Score: %.2f%%\" % (f1_score(y_test.values.ravel(), y_pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhip\\Documents\\projects\\ida_project\\env\\lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py:336: FutureWarning: The parameter `n_jobs` has been deprecated in 0.10 and will be removed in 0.12. You can pass an nearest neighbors estimator where `n_jobs` is already set instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.20%\n",
      "F1 Score: 56.31%\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0, shuffle=True, stratify=y)\n",
    "\n",
    "classifier = AdaBoostClassifier(\n",
    "    n_estimators=50,\n",
    "    learning_rate=1.0,\n",
    "    algorithm='SAMME.R',\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# transformer = FunctionTransformer(func=np.log1p)\n",
    "# transformer = PowerTransformer()\n",
    "# transformer = QuantileTransformer(output_distribution='normal', random_state=0)\n",
    "# X_train[numerical_cols] = transformer.fit_transform(X_train[numerical_cols])\n",
    "# X_test[numerical_cols] = transformer.transform(X_test[numerical_cols])\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train[:] = scaler.fit_transform(X_train[:])\n",
    "# X_test[:] = scaler.transform(X_test[:])\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train[numerical_cols] = imputer.fit_transform(X_train[numerical_cols])\n",
    "X_test[numerical_cols] = imputer.transform(X_test[numerical_cols])\n",
    "\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "X_train[categorical_cols] = imputer.fit_transform(X_train[categorical_cols])\n",
    "X_test[categorical_cols] = imputer.transform(X_test[categorical_cols])\n",
    "\n",
    "over_under_sampler = SMOTEENN(random_state=0, n_jobs=-1)\n",
    "# over_under_sampler = SMOTETomek(random_state=0, n_jobs=-1)\n",
    "X_train, y_train = over_under_sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "classifier.fit(X_train, y_train.values.ravel())\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_score(y_test.values.ravel(), y_pred)*100))\n",
    "print(\"F1 Score: %.2f%%\" % (f1_score(y_test.values.ravel(), y_pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhip\\Documents\\projects\\ida_project\\env\\lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py:336: FutureWarning: The parameter `n_jobs` has been deprecated in 0.10 and will be removed in 0.12. You can pass an nearest neighbors estimator where `n_jobs` is already set instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 68.50%\n",
      "F1 Score: 57.14%\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0, shuffle=True, stratify=y)\n",
    "\n",
    "classifier = BaggingClassifier(\n",
    "    n_estimators=125,\n",
    "    max_samples=0.9,\n",
    "    max_features=0.9,\n",
    "    bootstrap=True,\n",
    "    bootstrap_features=True,\n",
    "    oob_score=False,\n",
    "    n_jobs=-1,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# transformer = FunctionTransformer(func=np.log1p)\n",
    "# transformer = PowerTransformer()\n",
    "# transformer = QuantileTransformer(output_distribution='normal', random_state=0)\n",
    "# X_train[numerical_cols] = transformer.fit_transform(X_train[numerical_cols])\n",
    "# X_test[numerical_cols] = transformer.transform(X_test[numerical_cols])\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train[:] = scaler.fit_transform(X_train[:])\n",
    "# X_test[:] = scaler.transform(X_test[:])\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train[numerical_cols] = imputer.fit_transform(X_train[numerical_cols])\n",
    "X_test[numerical_cols] = imputer.transform(X_test[numerical_cols])\n",
    "\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "X_train[categorical_cols] = imputer.fit_transform(X_train[categorical_cols])\n",
    "X_test[categorical_cols] = imputer.transform(X_test[categorical_cols])\n",
    "\n",
    "over_under_sampler = SMOTEENN(random_state=0, n_jobs=-1)\n",
    "# over_under_sampler = SMOTETomek(random_state=0, n_jobs=-1)\n",
    "X_train, y_train = over_under_sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "classifier.fit(X_train, y_train.values.ravel())\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_score(y_test.values.ravel(), y_pred)*100))\n",
    "print(\"F1 Score: %.2f%%\" % (f1_score(y_test.values.ravel(), y_pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhip\\Documents\\projects\\ida_project\\env\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "c:\\Users\\abhip\\Documents\\projects\\ida_project\\env\\lib\\site-packages\\xgboost\\data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:40:39] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Accuracy: 73.07%\n",
      "F1 Score: 58.80%\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0, shuffle=True, stratify=y)\n",
    "\n",
    "classifier = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate = 0.3,\n",
    "    n_jobs=-1,\n",
    "    scale_pos_weight=59145/23855,\n",
    "    random_state=0,\n",
    ")\n",
    "\n",
    "# transformer = FunctionTransformer(func=np.log1p)\n",
    "# transformer = PowerTransformer()\n",
    "# transformer = QuantileTransformer(output_distribution='normal', random_state=0)\n",
    "# X_train[numerical_cols] = transformer.fit_transform(X_train[numerical_cols])\n",
    "# X_test[numerical_cols] = transformer.transform(X_test[numerical_cols])\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train[:] = scaler.fit_transform(X_train[:])\n",
    "# X_test[:] = scaler.transform(X_test[:])\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_train[numerical_cols] = imputer.fit_transform(X_train[numerical_cols])\n",
    "X_test[numerical_cols] = imputer.transform(X_test[numerical_cols])\n",
    "\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "X_train[categorical_cols] = imputer.fit_transform(X_train[categorical_cols])\n",
    "X_test[categorical_cols] = imputer.transform(X_test[categorical_cols])\n",
    "\n",
    "# over_under_sampler = SMOTEENN(random_state=0, n_jobs=-1)\n",
    "# over_under_sampler = SMOTETomek(random_state=0, n_jobs=-1)\n",
    "# X_train, y_train = over_under_sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "classifier.fit(X_train, y_train.values.ravel())\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_score(y_test.values.ravel(), y_pred)*100))\n",
    "print(\"F1 Score: %.2f%%\" % (f1_score(y_test.values.ravel(), y_pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 72.26%\n",
      "F1 Score: 59.20%\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0, shuffle=True, stratify=y)\n",
    "\n",
    "classifier = LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    n_jobs=-1,\n",
    "    scale_pos_weight=59145/23855,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# transformer = FunctionTransformer(func=np.log1p)\n",
    "transformer = PowerTransformer()\n",
    "# transformer = QuantileTransformer(output_distribution='normal', random_state=0)\n",
    "X_train[numerical_cols] = transformer.fit_transform(X_train[numerical_cols])\n",
    "X_test[numerical_cols] = transformer.transform(X_test[numerical_cols])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train[:] = scaler.fit_transform(X_train[:])\n",
    "X_test[:] = scaler.transform(X_test[:])\n",
    "\n",
    "# imputer = SimpleImputer(strategy='median')\n",
    "# X_train[numerical_cols] = imputer.fit_transform(X_train[numerical_cols])\n",
    "# X_test[numerical_cols] = imputer.transform(X_test[numerical_cols])\n",
    "\n",
    "# imputer = SimpleImputer(strategy='most_frequent')\n",
    "# X_train[categorical_cols] = imputer.fit_transform(X_train[categorical_cols])\n",
    "# X_test[categorical_cols] = imputer.transform(X_test[categorical_cols])\n",
    "\n",
    "# over_under_sampler = SMOTEENN(random_state=0, n_jobs=-1)\n",
    "# over_under_sampler = SMOTETomek(random_state=0, n_jobs=-1)\n",
    "# X_train, y_train = over_under_sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "classifier.fit(X_train, y_train.values.ravel())\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy_score(y_test.values.ravel(), y_pred)*100))\n",
    "print(\"F1 Score: %.2f%%\" % (f1_score(y_test.values.ravel(), y_pred)*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:39:17) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7d873a7aaaf2e36eff25da74ca9b121e73dd5f9a6b34e9af2342396190b76968"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
